# Generative AI Security Papers:

## Large Language/Reasoning Models Safety

### Survey | Measurements | Benchmarks

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| The Digital Cybersecurity Expert: How Far Have We Come? | [Venus](https://arxiv.org/abs/2504.11783) | [PDF](https://arxiv.org/pdf/2504.11783) | - |
| Safety in Large Reasoning Models: A Survey | [Venus](https://arxiv.org/abs/2504.17704) | [PDF](https://arxiv.org/pdf/2504.17704) | - |
| Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models | [Venus](https://arxiv.org/abs/2503.16419) | [PDF](https://arxiv.org/pdf/2503.16419) | - |
| From System 1 to System 2: A Survey of Reasoning Large Language Models | [Venus](https://arxiv.org/abs/2502.17419) | [PDF](https://arxiv.org/pdf/2502.17419) | - |
| Safety at Scale: A Comprehensive Survey of Large Model Safety | [Venus](https://arxiv.org/abs/2502.05206) | [PDF](https://arxiv.org/pdf/2502.05206) | - |
| Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies | [Venus](https://arxiv.org/abs/2501.17030) | [PDF](https://arxiv.org/pdf/2501.17030v1) | - |
| Reasoning Models Don't Always Say What They Think | - | [PDF](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) | - |
| The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1 | [Venus](https://arxiv.org/abs/2502.12659) | [PDF](https://arxiv.org/pdf/2502.12659) | - |
| SAFECHAIN: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities | [Venus](https://arxiv.org/abs/2502.12025) | [PDF](https://arxiv.org/pdf/2502.12025) | - |
| DeepSeek-R1 Thoughtology: Let's \<think\> about LLM Reasoning | [Venus](https://arxiv.org/abs/2504.07128) | [PDF](https://arxiv.org/pdf/2504.07128) | - |
| o3-mini vs DeepSeek-R1: Which One is Safer? | [Venus](https://arxiv.org/abs/2501.18438) | [PDF](https://arxiv.org/pdf/2501.18438) | - |
| Towards Understanding the Safety Boundaries of DeepSeek Models: Evaluation and Findings | [Venus](https://arxiv.org/abs/2503.15092) | [PDF](https://arxiv.org/pdf/2503.15092) | - |
| Safety Evaluation of DeepSeek Models in Chinese Contexts | [Venus](https://arxiv.org/abs/2502.11137) | [PDF](https://arxiv.org/pdf/2502.11137) | - |
| SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models | [Venus](https://arxiv.org/abs/2504.08813) | [PDF](https://arxiv.org/pdf/2504.08813) | - |
| Are Smarter LLMs Safer? Exploring Safety-Reasoning Trade-offs in Prompting and Fine-Tuning | [Venus](https://arxiv.org/abs/2502.09673) | [PDF](https://arxiv.org/pdf/2502.09673) | - |

### Attacks

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| OverThink: Slowdown Attacks on Reasoning LLMs | [Venus](https://arxiv.org/abs/2502.02542) | [PDF](https://arxiv.org/pdf/2502.02542) | - |
| Trading Inference-Time Compute for Adversarial Robustness | [Venus](https://arxiv.org/abs/2501.18841) | [PDF](https://arxiv.org/pdf/2501.18841) | - |
| A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos | [Venus](https://arxiv.org/abs/2502.15806) | [PDF](https://arxiv.org/pdf/2502.15806) | - |
| H-CoT: Hijacking the Chain-of-Thought Safety Reasoning Mechanism to Jailbreak Large Reasoning Models | [Venus](https://arxiv.org/abs/2502.12893) | [PDF](https://arxiv.org/pdf/2502.12893) | - |
| Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps | [Venus](https://arxiv.org/abs/2503.19326) | [PDF](https://arxiv.org/pdf/2503.19326) | - |
| ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs | [Venus](https://arxiv.org/abs/2504.05605) | [PDF](https://arxiv.org/pdf/2504.05605) | - |
| BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack | [Venus](https://arxiv.org/abs/2502.12202) | [PDF](https://arxiv.org/pdf/2502.12202) | - |
| DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs | [Venus](https://arxiv.org/abs/2501.18617) | [PDF](https://arxiv.org/pdf/2501.18617) | - |
| SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models | [Venus](https://arxiv.org/abs/2504.08813) | [PDF](https://arxiv.org/pdf/2504.08813) | - |
| Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models | [Venus](https://arxiv.org/abs/2502.11054) | [PDF](https://arxiv.org/pdf/2502.11054) | - |
| Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues | [Venus](https://arxiv.org/abs/2410.10700) | [PDF](https://arxiv.org/pdf/2410.10700) | - |
| LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet | [Venus](https://arxiv.org/abs/2408.15221) | [PDF](https://arxiv.org/pdf/2408.15221) | - |

### Defenses

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| STAR-1: Safer Alignment of Reasoning LLMs with 1K Data | [Venus](https://arxiv.org/abs/2504.01903) | [PDF](https://arxiv.org/pdf/2504.01903) | - |
| RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability | [Venus](https://arxiv.org/abs/2504.10081) | [PDF](https://arxiv.org/pdf/2504.10081) | - |
| Leveraging Reasoning with Guidelines to Elicit and Utilize Knowledge for Enhancing Safety Alignment | [Venus](https://arxiv.org/abs/2502.12025) | [PDF](https://arxiv.org/pdf/2502.12025) | - |
| Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning | [Venus](https://arxiv.org/abs/2501.19180) | [PDF](https://arxiv.org/pdf/2501.19180) | - |

## AI Agent Security

### Survey | Measurements | Benchmarks

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| AGENT-SAFETYBENCH: Evaluating the Safety of LLM Agents | [Venus](https://arxiv.org/abs/2412.14470) | [PDF](https://arxiv.org/pdf/2412.14470) | - |
| A Survey on Trustworthy LLM Agents: Threats and Countermeasures | [Venus](https://arxiv.org/abs/2503.09648) | [PDF](https://arxiv.org/pdf/2503.09648) | - |
| R-Judge: Benchmarking Safety Risk Awareness for LLM Agents | [Venus](https://arxiv.org/abs/2401.10019) | [PDF](https://arxiv.org/pdf/2401.10019) | - |
| AI Agents Under Threat: A Survey of Key Security Challenges | [Venus](https://arxiv.org/abs/2406.02630) | [PDF](https://arxiv.org/pdf/2406.02630) | - |
| Emerging Cyber Attack Risks of Medical AI Agents | [Venus](https://arxiv.org/abs/2504.03759) | [PDF](https://arxiv.org/pdf/2504.03759) | - |
| Agentdojo: A dynamic environment to evaluate attacks and defenses for llm agents | [Venus](https://arxiv.org/abs/2406.13352) | [PDF](https://arxiv.org/pdf/2406.13352) | - |
| Formalizing and benchmarking attacks and defenses in llm-based agents | [Venus](https://arxiv.org/abs/2410.02644) | [PDF](https://arxiv.org/abs/2410.02644) | - |
| Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents | [Venus](https://arxiv.org/abs/2502.11355) | [PDF](https://arxiv.org/abs/2502.11355) | - |
| RedCode: Risky Code Execution and Generation Benchmark for Code Agents | - | [PDF](https://proceedings.neurips.cc/paper_files/paper/2024/file/bfd082c452dffb450d5a5202b0419205-Paper-Datasets_and_Benchmarks_Track.pdf) | - |
| CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities | [Venus](https://arxiv.org/abs/2503.17332) | [PDF](https://arxiv.org/abs/2503.17332) | - |
| Security of AI Agents | [Venus](https://arxiv.org/abs/2406.08689) | [PDF](https://arxiv.org/pdf/2406.08689) | - |
| Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents | [Venus](https://arxiv.org/abs/2411.09523) | [PDF](https://arxiv.org/pdf/2411.09523) | - |

### Attacks

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning | [Venus](https://arxiv.org/abs/2503.01908) | [PDF](https://arxiv.org/abs/2503.01908) | - |
| Towards Action Hijacking of Large Language Model-based Agent | [Venus](https://arxiv.org/abs/2412.10807) | [PDF](https://arxiv.org/pdf/2412.10807) | - |

### Defenses

| Paper | Venus | PDF | Code |
|-------|-------|-----|------|
| SHIELDAGENT: Shielding Agents via Verifiable Safety Policy Reasoning | [Venus](https://arxiv.org/abs/2503.22738) | [PDF](https://arxiv.org/pdf/2503.22738) | - |
| Defining and Detecting the Defects of the Large Language Model-based Autonomous Agents | [Venus](https://arxiv.org/abs/2412.18371) | [PDF](https://arxiv.org/pdf/2412.18371) | - |
| PentestAgent: Incorporating LLM Agents to Automated Penetration Testing | [Venus](https://arxiv.org/abs/2411.05185) | [PDF](https://arxiv.org/pdf/2411.05185) | - |
